{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction The term storage can mean different things to different people depending on one's perspective. An application developer who needs some storage could mean object storage aka a database in which they can store some data. That same application developer who needs to store same raw data on a disk could mean that the need file storage . This is just a filesystem where random files can be stored. When an operations person talks about storage, they typically mean block storage . That would be a chunck of raw storage which could be formatted with any kind of operating system depending the need. That same operations person could mean virtual storage which is essentially a file which represents the hard disk of a virtual machine. When speaking with a storage engineer one could be talking about physical storage - a Direct Access Storage Device (DASD) such is a mechanical disk, or SSD/NVMe/flash drive. That same engineer could be talking about a SAN based storage array such as an IBM Elastic Storage Server or XIV. In reality all of these things are storage and all are important when considering how to provide some space where an application can store some data which is the ultimate goal. This document will describe all aspects of providing storage for cloud based applications including recommendations for low level infrastructure all the way up to object storage databases. Part I will talk about all of the various concepts associated with providing cloud storage. Part II will provide specific recipes describing how to implement various cloud storage technologies. Appendix A will provide performance benchmarks for various types of storage. These benchmarks will carry out the exact same operations on various storage technologies and provide key performance indicators which can be compared to determine the best solution for your storage requirements.","title":"Home"},{"location":"#introduction","text":"The term storage can mean different things to different people depending on one's perspective. An application developer who needs some storage could mean object storage aka a database in which they can store some data. That same application developer who needs to store same raw data on a disk could mean that the need file storage . This is just a filesystem where random files can be stored. When an operations person talks about storage, they typically mean block storage . That would be a chunck of raw storage which could be formatted with any kind of operating system depending the need. That same operations person could mean virtual storage which is essentially a file which represents the hard disk of a virtual machine. When speaking with a storage engineer one could be talking about physical storage - a Direct Access Storage Device (DASD) such is a mechanical disk, or SSD/NVMe/flash drive. That same engineer could be talking about a SAN based storage array such as an IBM Elastic Storage Server or XIV. In reality all of these things are storage and all are important when considering how to provide some space where an application can store some data which is the ultimate goal. This document will describe all aspects of providing storage for cloud based applications including recommendations for low level infrastructure all the way up to object storage databases. Part I will talk about all of the various concepts associated with providing cloud storage. Part II will provide specific recipes describing how to implement various cloud storage technologies. Appendix A will provide performance benchmarks for various types of storage. These benchmarks will carry out the exact same operations on various storage technologies and provide key performance indicators which can be compared to determine the best solution for your storage requirements.","title":"Introduction"},{"location":"architecture/","text":"Architecture","title":"Appendix B - Physical Storage Architecture Considerations"},{"location":"architecture/#architecture","text":"","title":"Architecture"},{"location":"benchmarks/","text":"Introduction Local Storage vs Remote Storage Remote Storage Technologies Local Block Storage Introduction Bare Metal VMware KVM Cloud Based Block Storage Introduction Ceph GlusterFS NFS IBM Spectrum Scale Amazon S3 Object Storage Introduction minio mariaDB mongoDB CleverSafe (local) CouchDB/Cloudant (local) Redis (local)","title":"Appendix A - Performance Benchmarks"},{"location":"benchmarks/#introduction","text":"Local Storage vs Remote Storage Remote Storage Technologies","title":"Introduction"},{"location":"benchmarks/#local-block-storage","text":"Introduction Bare Metal VMware KVM","title":"Local Block Storage"},{"location":"benchmarks/#cloud-based-block-storage","text":"Introduction Ceph GlusterFS NFS IBM Spectrum Scale Amazon S3","title":"Cloud Based Block Storage"},{"location":"benchmarks/#object-storage","text":"Introduction minio mariaDB mongoDB CleverSafe (local) CouchDB/Cloudant (local) Redis (local)","title":"Object Storage"},{"location":"concepts/","text":"Storage Types Physical Storage The physical storage which is used to back cloud storage technologies is important, especially in a kubernetes environment. kuberetes master nodes utilize an etcd database and in an HA environment, at least three different instances of etcd. etcd is a high IOPS (Input/Output Operations Per Second) application which writes a large volume of data to the disk. The performance of this database is critical to the performance of the cluster and the larger the cluster, the more data gets written For this reason, it is critical that the storage backing the filesystems where the etcd database is written be the fastest storage technology available. It should be noted that one need not necessarily implement NVMe drives to get the best performance. Spreading the IOPS across hundreds of spindles on mechanical drives (such as is done with IBM Elastic Storage Servers) can also be very fast. Additionally, running NVMe drives with a slow virtual storage technology can render them no faster than a mechanical disk. For more information see Appendix A . See the On-Premesis Storage Infrastructure below for more infrastruction on recommendations on how to properly configure physical storage infrastructure for maximum performance. Block Storage Block storage technologies provide raw chunks of storage which usually must be formatted before they can be used. One example of block storage would be a LUN/Volume created on a SAN storage device which can be mapped to a remote host and appears to that host as a physical disk. Another example would be a Ceph RBD volume. Once created and mapped to a host, such a disk shows up on the filesystem like any other disk. In both of these cases the block storage volume must be formatted with a filesystem before it can be used. This detail is hidden in the Ceph kubernetes storage provider because the Ceph storage driver will put a filesystem on the block device prior to mounting to the container. It is still a block device, it has just been formatted prior to use. File Storage File storage is a storage device which is provided pre-formatted with a file system prior to making it available to be mounted. Probaly the best example of this type of storage is NFS (Network File System). NFS filesytems are exported by an NFS server and have already been formatted before they are exported. Clients that mount these filesystems do not have to format the device prior to use. Another good example of file storage would be CephFS. Object Storage Object storage is simply a database which has some kind of an API which can be used for storing chunks of data, either formatted or unformatted. Object storage need not be mounted to a filesystem to be used, data is normally storage via REST API calls or SQL commands. In cloud native applications, it is very common to deploy a separate module/pod/container which contains a database which can then be exposed to the application for object storage. In this case, the container hosting the object database will likely need file or block storage to hold the database files, but the application itself will consume the objec storage provided by the database. In other cases, applications could consume object storage which is hosted somewhere out on the internet or on some legacy physical infrastructure. Examples of object storage technologies would be Redis, CouchDB/Cloudant, minio, mongoDB, mariaDB, etc. Example of hosted object storage technologies woudl be cloudant.com, redis.com, or an existing DB2 database hosted on legacy infrastructure in the enterprise. On-Premesis Storage Infrastructure SAN (Storage Area Network) vs Converged Networking There has been a significant amount of discussion around the topic of converged or hyper-converted networks. A converged network is one where data and storage network traffic are combined onto a single infrastructure. The topic is much more broad than this simple statement, but this is the aspect that is of most concern to the cloud storage topic. Experience shows that a converged or hyper-conconverged infrastructure may or may not provide better performance based on a number of factors. We will not attempt to make a recommendation on whether a company should use SAN or a converged or hyper-converged infrastructure, but one thing that will be clear from looking at the benchmarks provided in Appendix A is that what is advertised to be better, faster technology may not be if it is not implemented in the right way. Prior to choosing a storage technology it is highly recommended that proper performance testing be performed to ensure that the architecture to be implemented provides the performance that is desired and expected. Experience shows that storage provider technologies which provide block storage over the data network (such as Ceph or Gluster) consume a significant number of CPU cycles serving up disk I/O over a network which is designed for traditional data. SAN storage technologies are designed for disk I/O traffic and the performance of an 8GB SAN will greatly out-perform a 10GB data network in a converged environment. Storage Network Congestion in a kubernetes Environment Storage traffic patterns in a kubernetes environment is significantly different than in a traditional physical or even virtual environment. Historically, physical infrastructures were easy enough to understand because each physical machine was an endpoint and occupied one port on a SAN switch. Communication paths over the SAN are well known and constant. The advent of virtual networks changed things a bit in that in a virtual environment you could now have many more volumes mapped to a single physical host. In an HPC (High Performance Computing) environment, a single physical node could have dozens or more virtual machines, each of which could have a volume served up over the SAN. Ultimately, however, virtual machines do not move around very much and when they do, they move to other physical machines utilizing the same mount points which were setup ahead of time and do not change. In the world of kubernetes, however, workloads are moved around in the infrastructure and scale out and back with regularity. As workloads move to different worker nodes these volumes are moved around to various machines. This leads to a situation where storage traffic can become significantly unbalanced on the SAN and a single compute node could potentially have hundreds of endpoints depending on the number of containers running on the node. SAN networks can become congested not only because of high traffic, but also because of a high number of endpoints behind a single SAN switch port. In an HPC environment where a single physical node could host dozens of virtual machines and each virtual machine could be hosting hundreds or even thousands of endpoints in a dynamically provisoined environment, network congestion can become a significant problem. When the SAN network becomes congested, it can backup across the SAN infrastructure negatively impacting completely unrelated workloads. In relatively small kubernetes environments, this type of congestion is normally not a problem, however, large clusters with hundreds or thousands of worker nodes or an environment which is hosting dozens or hundreds of smaller clusters, significant network congestion can be a significant problem, especially when workload storage is being provided inside the cluster itself (vs externally hosted cloud storage). See Appendix B below for recommendations on properly architecting a SAN infrastructure which can handle the extremely dynamic workloads that are standard in a kubernetes environment. Hosted Storage Infrastructure One option for deploying kubernetes clusters is to deploy them into a hosted Infrastructure-as-a-Service (IaaS) provider such is IBM Cloud or Amazon Web Services (AWS). Implmenting workload storage in these environments allows the developer to consume local storage which is provided by the cloud provider. Storage hosted by these cloud providers can be either block storage on which you can install whatever storage technology you like, or object storage to which you can write data directly via an API. Hosted storage can normally be purchased with various IOPS so you can tailor the storage back-end to the type of storage you need. In such an environment the operator has very little control over how the underlying physical infrastructure is architected. This should be considered if planning to implement large scale kubernetes environments in a hosted environment (see Storage Network Congestion in a kubernetes Environment above). Hosted Block Storage IBM Cloud Storage Amazon S3 Hosted Object Storage Cloudant.com Redis.com Resilience, Performance, and Scalability Replication vs Distribution Data resilience in a legacy environment is normally dependant upon replication. This means using data syncing technologies to replicate data between databases or storage devices. This kind of resiliency plan, however, can be extremely expensive requiring duplicates of all the physical infrastructure used to keep these replicas. As a result, many companies are willing to settle for backup technologies to keep offline copies of critical data. These backup storage technologies are typically much less expensive than the online replication technologies, but an outage could result in data loss between the time of the outage an the last time a backup was made. Cloud Native technologies, however, handle resilience in a different way. Object storage technologies such is IBM's CleverSafe break the data up into chunks and store slices across multiple physical devices or even datacenters. With many nodes running in many environments in different geographys, data is secure and resilient so long as a quorum of nodes is up and available. So, if the CleverSafe infrastructure is made up of 12 nodes, as many as 5 could faile with no data loss. If nodes are running in separate geographies or at least separate physical locations, the likelihood of losing more than half of the total nodes is extremely low. It is highly recommended that applications utilize modern cloud native storage technologies to maximize resilience at minimal cost. It should be noted that, whereas this type of technology provides for extreme availability and resilience, it does not protect against data corruption. If garbage is written to the database the database contains garbage and absent some additional procedures and planning, there is no way to reverse it. This means that there still is a good use case for making regular backups of data. The important thing here, though, is that in a kubernetes environment, application data can be backed up at the workload storage location vs backing up the entire cluster and everything on it, significantly reducing the amount of space needed for a proper backup. CAUTION: When providing internal storage technologies within a kubernetes cluster (e.g. Ceph or GlusterFS), the more bricks/OSDs you provide the more resilient your infrastructure is likely to be. Make sure to Use anti-affinity rules to make sure each of the nodes hosting this storage is running on separte physical nodes and each of the bricks/OSDs are backed by separate physical infrastructure. If all nodes or a majority of nodes are running on a single physical host and that host fails, your storage techonogy will also fail due to a lack of enough backing storage to complete a transaction. This could lead to data loss or corruption. IOPS Considerations As noted above, kubernetes master nodes running etcd require significantly higher IOPS than most other nodes. High IOPS storage is typically also much more expensive than lower IOPS storage. Since only the etcd database needs to be stored on high IOPS storage, that expensive storage utilization can be minimized by mounting only the path on the disk where the etcd database is stored from high IOPS storage and leaving the rest of the master nodes backed by lower and less expensive IOPS storage. Some time sensitive workloads will also need high IOPS storage. It is recommended to provide multiple storage classes at various tiers so the developer can choose the storage that best supports the requirements of the workload. kubernetes Workload Scalability's affects on Storage When creating an application architecture, some developers may consider using a ReadWriteMany persistent Volume (see kubenetes Storage Concepts below for more information on persistent volumes) when they need multiple micro services to have access to the same persistent storage data. Caution should be used, however, because if using appliation auto-scaling in kubernetes, when an application scales out each container in each pod with a ReadWriteMany persistent volume will have that persistent volume mounted. This could lead to storage network congestion negatively impacting not only the entire kubernetes cluster, but also everything else running on the SAN infrastructure (see Storage Network Congestion in a kubernetes Environment above). A better architecture utilizes a micro service with an API to serve up data from a single ReadWriteOnly persistent volume which is then consumed by all workloads that need access to that data. Kubernetes Storage Concepts Persistent Volumes and Persistent Volume Claims In kubernetes, the request for storage by an application is abstracted from the storage source. Other than some of the basic storage attributes discussed below the application does not typically know nor care where the storage comes from. Those details can be exposed to the application if needed for some reason, but typically, an application asks for the storage attributes it needs and the platform decides where it comes from. The chunk of storage made available for the application to consume is called a Persistent Volume or PV. When an application needs some persistent storage it creates a request called a Peristent Volume Claim or PVC. When presented with a PVC, the platform will find a PV that meets the need and then bind the PVC to the PV. Once a PV is bound, it is then unavailable to be bound to any other PVC unless it is a ReadWriteMany (discussed below) request which allows a single PV to be bound to many PVCs. Dynamic vs Static A static PV is one which is created ahead of time by a system operator who would typically create a number of different PVs with different types of attributes to account for various types of PVCs that may want to consume them. A dynamic PV, however, is one which is created on demand . With a dynamic storage provider, when an application creates a PVC request the storage provider will create a PV that meets all of the requirements of the PVC and the platform will bind it to the PVC on the fly. This precludes the need for manually creating a PV for any given storage request. Data Retention Modes One of the requirements of a PV is its Data Retention Mode . This describes what happens to the data when the PVC which is bound to it is deleted - such as when an application is deleted from the cluster. Retain - If the retention mode is set to retain the PV is not deleted and no data on the PV is deleted. This is typically used when an application is uninstalled to be replaced by a newer version and the data should be retained between installations. It could also be used to make sure that the data in the PV is backed up before being removed. IMPORTANT: A PV with a retention mode set to retain is never removed by the system and must be manually removed when it is no longer needed. If this manual removal never happens and many applications are deployed this way, it could result in significant storage utilization growth over time. As a result, this retention mode should be used with caution. Delete - A PV with a retention mode set to delete will cause the PV to be deleted when the PVC that is bound to it is deleted. This will result in the loss of any data which exists on the PV when it is deleted. This is typically only used with dynamically created PVs. Recycle - When a PV has a retention mode of recycle the platform will try to remove any data on the PV and put it back into the pool to be bound to another PVC at some future time. WARNING: When a PV has a recycle retention mode the platform will execute an rm -rf / on the root of the PV. If the PV is an NFS volume and the path of the NFS mount is the root of the NFS server it will effectively wipe the NFS server. If the PV is a hostPath (a path on the local disk) and the path is set to / , the platform will wipe the entire local disk. Usage of this retention mode should be used with extreme caution and it is highly recommended that hostPath storage not be used with a kubernetes cluster. Access Modes Storage Access Modes define how a pod will use the PV. Note that the smallest unit of control in a kubernetes environment is a pod. If a PV is mounted to a pod it is mounted to all containers in the pod. ReadOnlyMany (ROX) - Analogous to a CD-ROM. PVs with this access mode can be mounted read-only by any number of pods, but none can write to it. It can be useful for providing access to certification keys or common software or document repositories, etc. ReadWriteOnce (RWO) - Only one pod can mount the PV at a time, but that pod can read from and write to it. ReadWriteMany (RWX) - Many pods can mount the PV and all can read and write to it. This access mode is not supported by many storage providers because of the requirement to keep all writers in sync to prevent race conditions. Any application that utilizes RWX access mode PVs is responsible for managing coordinated writes to prevent data corruption or loss. Storage Classes All kubernetes storage is made available via storage providers and there are quite a few storage providers available: hostPath, NFS, Ceph, Gluster, vSphere, just to name a very few. Different storage providers will support different attributes for the PVs it controls. Before choosing a storage provider the attributes that are supported should be considered. The way a storage provider is utilized is through a storage class. A storage class defines all the parameters needed by the storage provider to create a PV. The specific attributes needed for the storage class depends on the storage provider. It is common for a storage class name to include information about the storage provider and storage other attributes of the storage provider. For instance, a platform could have a storage class named \"ceph-fast\" indicating that if a PVC requests a PV created by this storage class it will be provided by the ceph storage provider backed by high IOPS storage. An operator may want to be even more descriptive and name the storage class \"ceph-tier0\" or \"ceph-flash\". Operators are advised to be caution using too much detail and creating too many different types of storage classes due to the risk of the developer not knowing what they all mean and chosing the wrong type using expensive storage when they only needed the the less expensive type. T-Shirt sizes (fast, medium, slow) seems to be a good way to label storage classes.","title":"Part I - Cloud Storage Concepts"},{"location":"concepts/#storage-types","text":"Physical Storage The physical storage which is used to back cloud storage technologies is important, especially in a kubernetes environment. kuberetes master nodes utilize an etcd database and in an HA environment, at least three different instances of etcd. etcd is a high IOPS (Input/Output Operations Per Second) application which writes a large volume of data to the disk. The performance of this database is critical to the performance of the cluster and the larger the cluster, the more data gets written For this reason, it is critical that the storage backing the filesystems where the etcd database is written be the fastest storage technology available. It should be noted that one need not necessarily implement NVMe drives to get the best performance. Spreading the IOPS across hundreds of spindles on mechanical drives (such as is done with IBM Elastic Storage Servers) can also be very fast. Additionally, running NVMe drives with a slow virtual storage technology can render them no faster than a mechanical disk. For more information see Appendix A . See the On-Premesis Storage Infrastructure below for more infrastruction on recommendations on how to properly configure physical storage infrastructure for maximum performance. Block Storage Block storage technologies provide raw chunks of storage which usually must be formatted before they can be used. One example of block storage would be a LUN/Volume created on a SAN storage device which can be mapped to a remote host and appears to that host as a physical disk. Another example would be a Ceph RBD volume. Once created and mapped to a host, such a disk shows up on the filesystem like any other disk. In both of these cases the block storage volume must be formatted with a filesystem before it can be used. This detail is hidden in the Ceph kubernetes storage provider because the Ceph storage driver will put a filesystem on the block device prior to mounting to the container. It is still a block device, it has just been formatted prior to use. File Storage File storage is a storage device which is provided pre-formatted with a file system prior to making it available to be mounted. Probaly the best example of this type of storage is NFS (Network File System). NFS filesytems are exported by an NFS server and have already been formatted before they are exported. Clients that mount these filesystems do not have to format the device prior to use. Another good example of file storage would be CephFS. Object Storage Object storage is simply a database which has some kind of an API which can be used for storing chunks of data, either formatted or unformatted. Object storage need not be mounted to a filesystem to be used, data is normally storage via REST API calls or SQL commands. In cloud native applications, it is very common to deploy a separate module/pod/container which contains a database which can then be exposed to the application for object storage. In this case, the container hosting the object database will likely need file or block storage to hold the database files, but the application itself will consume the objec storage provided by the database. In other cases, applications could consume object storage which is hosted somewhere out on the internet or on some legacy physical infrastructure. Examples of object storage technologies would be Redis, CouchDB/Cloudant, minio, mongoDB, mariaDB, etc. Example of hosted object storage technologies woudl be cloudant.com, redis.com, or an existing DB2 database hosted on legacy infrastructure in the enterprise.","title":"Storage Types"},{"location":"concepts/#on-premesis-storage-infrastructure","text":"SAN (Storage Area Network) vs Converged Networking There has been a significant amount of discussion around the topic of converged or hyper-converted networks. A converged network is one where data and storage network traffic are combined onto a single infrastructure. The topic is much more broad than this simple statement, but this is the aspect that is of most concern to the cloud storage topic. Experience shows that a converged or hyper-conconverged infrastructure may or may not provide better performance based on a number of factors. We will not attempt to make a recommendation on whether a company should use SAN or a converged or hyper-converged infrastructure, but one thing that will be clear from looking at the benchmarks provided in Appendix A is that what is advertised to be better, faster technology may not be if it is not implemented in the right way. Prior to choosing a storage technology it is highly recommended that proper performance testing be performed to ensure that the architecture to be implemented provides the performance that is desired and expected. Experience shows that storage provider technologies which provide block storage over the data network (such as Ceph or Gluster) consume a significant number of CPU cycles serving up disk I/O over a network which is designed for traditional data. SAN storage technologies are designed for disk I/O traffic and the performance of an 8GB SAN will greatly out-perform a 10GB data network in a converged environment. Storage Network Congestion in a kubernetes Environment Storage traffic patterns in a kubernetes environment is significantly different than in a traditional physical or even virtual environment. Historically, physical infrastructures were easy enough to understand because each physical machine was an endpoint and occupied one port on a SAN switch. Communication paths over the SAN are well known and constant. The advent of virtual networks changed things a bit in that in a virtual environment you could now have many more volumes mapped to a single physical host. In an HPC (High Performance Computing) environment, a single physical node could have dozens or more virtual machines, each of which could have a volume served up over the SAN. Ultimately, however, virtual machines do not move around very much and when they do, they move to other physical machines utilizing the same mount points which were setup ahead of time and do not change. In the world of kubernetes, however, workloads are moved around in the infrastructure and scale out and back with regularity. As workloads move to different worker nodes these volumes are moved around to various machines. This leads to a situation where storage traffic can become significantly unbalanced on the SAN and a single compute node could potentially have hundreds of endpoints depending on the number of containers running on the node. SAN networks can become congested not only because of high traffic, but also because of a high number of endpoints behind a single SAN switch port. In an HPC environment where a single physical node could host dozens of virtual machines and each virtual machine could be hosting hundreds or even thousands of endpoints in a dynamically provisoined environment, network congestion can become a significant problem. When the SAN network becomes congested, it can backup across the SAN infrastructure negatively impacting completely unrelated workloads. In relatively small kubernetes environments, this type of congestion is normally not a problem, however, large clusters with hundreds or thousands of worker nodes or an environment which is hosting dozens or hundreds of smaller clusters, significant network congestion can be a significant problem, especially when workload storage is being provided inside the cluster itself (vs externally hosted cloud storage). See Appendix B below for recommendations on properly architecting a SAN infrastructure which can handle the extremely dynamic workloads that are standard in a kubernetes environment.","title":"On-Premesis Storage Infrastructure"},{"location":"concepts/#hosted-storage-infrastructure","text":"One option for deploying kubernetes clusters is to deploy them into a hosted Infrastructure-as-a-Service (IaaS) provider such is IBM Cloud or Amazon Web Services (AWS). Implmenting workload storage in these environments allows the developer to consume local storage which is provided by the cloud provider. Storage hosted by these cloud providers can be either block storage on which you can install whatever storage technology you like, or object storage to which you can write data directly via an API. Hosted storage can normally be purchased with various IOPS so you can tailor the storage back-end to the type of storage you need. In such an environment the operator has very little control over how the underlying physical infrastructure is architected. This should be considered if planning to implement large scale kubernetes environments in a hosted environment (see Storage Network Congestion in a kubernetes Environment above). Hosted Block Storage IBM Cloud Storage Amazon S3 Hosted Object Storage Cloudant.com Redis.com","title":"Hosted Storage Infrastructure"},{"location":"concepts/#resilience-performance-and-scalability","text":"Replication vs Distribution Data resilience in a legacy environment is normally dependant upon replication. This means using data syncing technologies to replicate data between databases or storage devices. This kind of resiliency plan, however, can be extremely expensive requiring duplicates of all the physical infrastructure used to keep these replicas. As a result, many companies are willing to settle for backup technologies to keep offline copies of critical data. These backup storage technologies are typically much less expensive than the online replication technologies, but an outage could result in data loss between the time of the outage an the last time a backup was made. Cloud Native technologies, however, handle resilience in a different way. Object storage technologies such is IBM's CleverSafe break the data up into chunks and store slices across multiple physical devices or even datacenters. With many nodes running in many environments in different geographys, data is secure and resilient so long as a quorum of nodes is up and available. So, if the CleverSafe infrastructure is made up of 12 nodes, as many as 5 could faile with no data loss. If nodes are running in separate geographies or at least separate physical locations, the likelihood of losing more than half of the total nodes is extremely low. It is highly recommended that applications utilize modern cloud native storage technologies to maximize resilience at minimal cost. It should be noted that, whereas this type of technology provides for extreme availability and resilience, it does not protect against data corruption. If garbage is written to the database the database contains garbage and absent some additional procedures and planning, there is no way to reverse it. This means that there still is a good use case for making regular backups of data. The important thing here, though, is that in a kubernetes environment, application data can be backed up at the workload storage location vs backing up the entire cluster and everything on it, significantly reducing the amount of space needed for a proper backup. CAUTION: When providing internal storage technologies within a kubernetes cluster (e.g. Ceph or GlusterFS), the more bricks/OSDs you provide the more resilient your infrastructure is likely to be. Make sure to Use anti-affinity rules to make sure each of the nodes hosting this storage is running on separte physical nodes and each of the bricks/OSDs are backed by separate physical infrastructure. If all nodes or a majority of nodes are running on a single physical host and that host fails, your storage techonogy will also fail due to a lack of enough backing storage to complete a transaction. This could lead to data loss or corruption. IOPS Considerations As noted above, kubernetes master nodes running etcd require significantly higher IOPS than most other nodes. High IOPS storage is typically also much more expensive than lower IOPS storage. Since only the etcd database needs to be stored on high IOPS storage, that expensive storage utilization can be minimized by mounting only the path on the disk where the etcd database is stored from high IOPS storage and leaving the rest of the master nodes backed by lower and less expensive IOPS storage. Some time sensitive workloads will also need high IOPS storage. It is recommended to provide multiple storage classes at various tiers so the developer can choose the storage that best supports the requirements of the workload. kubernetes Workload Scalability's affects on Storage When creating an application architecture, some developers may consider using a ReadWriteMany persistent Volume (see kubenetes Storage Concepts below for more information on persistent volumes) when they need multiple micro services to have access to the same persistent storage data. Caution should be used, however, because if using appliation auto-scaling in kubernetes, when an application scales out each container in each pod with a ReadWriteMany persistent volume will have that persistent volume mounted. This could lead to storage network congestion negatively impacting not only the entire kubernetes cluster, but also everything else running on the SAN infrastructure (see Storage Network Congestion in a kubernetes Environment above). A better architecture utilizes a micro service with an API to serve up data from a single ReadWriteOnly persistent volume which is then consumed by all workloads that need access to that data.","title":"Resilience, Performance, and Scalability"},{"location":"concepts/#kubernetes-storage-concepts","text":"Persistent Volumes and Persistent Volume Claims In kubernetes, the request for storage by an application is abstracted from the storage source. Other than some of the basic storage attributes discussed below the application does not typically know nor care where the storage comes from. Those details can be exposed to the application if needed for some reason, but typically, an application asks for the storage attributes it needs and the platform decides where it comes from. The chunk of storage made available for the application to consume is called a Persistent Volume or PV. When an application needs some persistent storage it creates a request called a Peristent Volume Claim or PVC. When presented with a PVC, the platform will find a PV that meets the need and then bind the PVC to the PV. Once a PV is bound, it is then unavailable to be bound to any other PVC unless it is a ReadWriteMany (discussed below) request which allows a single PV to be bound to many PVCs. Dynamic vs Static A static PV is one which is created ahead of time by a system operator who would typically create a number of different PVs with different types of attributes to account for various types of PVCs that may want to consume them. A dynamic PV, however, is one which is created on demand . With a dynamic storage provider, when an application creates a PVC request the storage provider will create a PV that meets all of the requirements of the PVC and the platform will bind it to the PVC on the fly. This precludes the need for manually creating a PV for any given storage request. Data Retention Modes One of the requirements of a PV is its Data Retention Mode . This describes what happens to the data when the PVC which is bound to it is deleted - such as when an application is deleted from the cluster. Retain - If the retention mode is set to retain the PV is not deleted and no data on the PV is deleted. This is typically used when an application is uninstalled to be replaced by a newer version and the data should be retained between installations. It could also be used to make sure that the data in the PV is backed up before being removed. IMPORTANT: A PV with a retention mode set to retain is never removed by the system and must be manually removed when it is no longer needed. If this manual removal never happens and many applications are deployed this way, it could result in significant storage utilization growth over time. As a result, this retention mode should be used with caution. Delete - A PV with a retention mode set to delete will cause the PV to be deleted when the PVC that is bound to it is deleted. This will result in the loss of any data which exists on the PV when it is deleted. This is typically only used with dynamically created PVs. Recycle - When a PV has a retention mode of recycle the platform will try to remove any data on the PV and put it back into the pool to be bound to another PVC at some future time. WARNING: When a PV has a recycle retention mode the platform will execute an rm -rf / on the root of the PV. If the PV is an NFS volume and the path of the NFS mount is the root of the NFS server it will effectively wipe the NFS server. If the PV is a hostPath (a path on the local disk) and the path is set to / , the platform will wipe the entire local disk. Usage of this retention mode should be used with extreme caution and it is highly recommended that hostPath storage not be used with a kubernetes cluster. Access Modes Storage Access Modes define how a pod will use the PV. Note that the smallest unit of control in a kubernetes environment is a pod. If a PV is mounted to a pod it is mounted to all containers in the pod. ReadOnlyMany (ROX) - Analogous to a CD-ROM. PVs with this access mode can be mounted read-only by any number of pods, but none can write to it. It can be useful for providing access to certification keys or common software or document repositories, etc. ReadWriteOnce (RWO) - Only one pod can mount the PV at a time, but that pod can read from and write to it. ReadWriteMany (RWX) - Many pods can mount the PV and all can read and write to it. This access mode is not supported by many storage providers because of the requirement to keep all writers in sync to prevent race conditions. Any application that utilizes RWX access mode PVs is responsible for managing coordinated writes to prevent data corruption or loss. Storage Classes All kubernetes storage is made available via storage providers and there are quite a few storage providers available: hostPath, NFS, Ceph, Gluster, vSphere, just to name a very few. Different storage providers will support different attributes for the PVs it controls. Before choosing a storage provider the attributes that are supported should be considered. The way a storage provider is utilized is through a storage class. A storage class defines all the parameters needed by the storage provider to create a PV. The specific attributes needed for the storage class depends on the storage provider. It is common for a storage class name to include information about the storage provider and storage other attributes of the storage provider. For instance, a platform could have a storage class named \"ceph-fast\" indicating that if a PVC requests a PV created by this storage class it will be provided by the ceph storage provider backed by high IOPS storage. An operator may want to be even more descriptive and name the storage class \"ceph-tier0\" or \"ceph-flash\". Operators are advised to be caution using too much detail and creating too many different types of storage classes due to the risk of the developer not knowing what they all mean and chosing the wrong type using expensive storage when they only needed the the less expensive type. T-Shirt sizes (fast, medium, slow) seems to be a good way to label storage classes.","title":"Kubernetes Storage Concepts"},{"location":"recipes/","text":"Note: The recipes in this document need a kubernetes (k8s) provider for the examples. This document will use IBM Cloud Private (ICP) as the k8s platform. Other providers will be slightly different, but the principles and general methodology should be the same for any k8s provider. Recipes may be provided for various linux flavors, again, the principles are the same, specific details may vary. Table of Contents Introduction Things to Consider Prior to Choosing a Storage Solution Recipes Integrating with an External NFS Server Deploying an External Ceph Instance and Integrating with ICP Deploying an Internal Ceph/Rook Instance Deploying an External Gluster Instance Deploying an Internal Gluster Instance Integrating with an existing VMware vCenter 6.5 Instance Integrating with an existing S3 storage provider Introduction Kubernetes can consume storage solutions deployed either as a part of a cluster (internal storage) or storage provided by an external service (external storage). Deploying a workload storage solution as a part of a cluster (internal storage) will limit access to the storage to workloads running inside the cluster. This will be more secure than deploying an external instance of the storage provider, but also limits who can consume the storage unless steps are taken to expose the storage outside the cluster. The use case will determine which you choose to deploy. If your objective is to have a large storage solution which can be consumed by multiple kubernetes clusters or non-kubernetes workloads, then your solution should include an instance of the storage provider which is external to the k8s cluster and then integrate your k8s instance with that storage provider via storage classes. If your objective is to have a secure storage solution which is only supporting a single k8s cluster, then you will want to deploy a storage provider inside the cluster and point your storage classes to the internal instance. Another alternative is to have a single k8s cluster which implements an internal storage provider but exposes the API service outside the cluster. This could then be used as a storage provider for other k8s clusters. It should be noted that the recipes in this document will describe implementing block and filesystem storage for workloads only (not platform storage) and, other than S3 storage providers, will not consider object storage. Things to Consider Prior to Choosing a Storage Solution As with all cloud resources, high availability is of primary concern. When creating a storage provider either internally or externally, the instance should be architected such that an entire node can fail and the storage provider can survive the failure. k8s is designed to be highly elastic and fungible. Nodes and processes can fail and k8s will just redeploy them elsewhere assuming adequate available resources. Consider a situation where an internal GlusterFS instance has been created in a smaller k8s environment and three storage nodes are provisioned. If the utilization of the overall storage environment reaches is high and the implementation of the storage provider does not have enough resources to spread the load well, data could be lost and/or the storage provider could go down altogether. Just like master nodes, storage nodes should run with anti-affinity rules such that no two storage nodes are running on the same physical infrastructure. This way the loss of a single physical node does not take out 2/3 of your available capacity. As a rule, a larger volume of storage nodes spread accross a larger number of physical nodes will be a better solution with a larger tolerance for failure. One should also avoid running multiple Ceph OSDs or Gluster Bricks on the same node. It does no good to have 9 GlusterFS bricks in your environment if they are running on only three nodes and one of the nodes fails. In this case, you have still lost 30% of your capacity with one failure. Different storage technologies can handle various amounts of loss (see product documentation for specific details for your chosen technology). A solution architect should be aware of those details and ensure whatever solution they choose to implement can handle the loss of at least one node. The number of nodes you choose to tolerate may be more than one based on business need, but any solution should take advantage of the traits of the k8s platform and ensure the storage infrastructure is properly architected. Recipes Recipe 1: Integrating with an External NFS server IMPORTANT: Whereas NFS is probably the easiest storage provider to configure for k8s and is useful in a development environment, IBM does not recommend using NFS for workload storage in production for a number of reasons. The standard NFS storage provider for k8s is not dynamic and requires NFS PVs be manually created (or scripted) and manual processes are prone to errors and can cause a delay for end users if no PVs exist with the needed attributes. This requires an operator to manually intervene in the DevOps process which is against basic DevOps principles. NFS is not secure. NFS is slow. If you need an NFS storage provider for dev purposes or it is required for some other reason, the following will help you make sure it is effective in a kubernetes environment. Creating the NFS Server Instructions for installing an NFS server differ slightly based on the operating system and instructions for doing so are ubiquitous on the internet. It is a fairly trivial exercise and will not be reproduced here. There are, however, a few recommendations for how to configure it once it is installed. Recommendations for the NFS server: Use LVM device mapping so that additional storage space can be added if needed. Do not give global mount permissions for NFS paths. This could allow different users on different clusters to mount the same path and overwrite each other. Each k8s cluster should have its own mount path. If sharing the NFS server with other non-k8s services, each other server should have its own mount permissions and mount path with no globol mount permissions. If a user can mount an NFS path via k8s and set the PV attribute to \"reclaim\", when the workload is removed k8s will run rm -rf / at the mount path which will result in permanently deleting anything on that path. Extreme caution should be exercised to prevent this possibility. Make sure attributes on the mount path include \"sync\" to prevent sync errors when consuming ReadWriteMany (RWX) PVs. admin@nfs-server:~$ cat /etc/exports /storage/cluster1 1.2.3.4(rw,no_subtree_check,sync,insecure,no_root_squash) Consuming NFS storage Create a new .yaml file with contents similar to the following: # myNfsVolume1.yaml kind: PersistentVolume apiVersion: v1 metadata: name: myNfsVolume1 spec: capacity: storage: 5Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle nfs: path: /storage/cluster1/myNfsVolume1 server: 1.2.3.4 This PV will have a total of 5GB of storage space, allow only one pod to have access, and when the deployment that is consuming it is deleted, its contents will be wiped and it will be placed back in the pool. Install your new PV using kubectl: kubectl create -f myNfsVolume1.yaml Recipe 2: Deploying an External Ceph Instance and Integrating with ICP Ceph is short for \"cephalopod\", a class of mollusks of which the octopus is a member. The octopus is used as the logo for Ceph and this name was chosen because the parallel processing nature of both the octopus and the software. There are two ways of integrating ICP with Ceph. The Ceph Rook open source project provides support for running the entire Ceph environment within a k8s cluster, utilizing local (hostpath) storage as raw disk for the OSD nodes. This method will install all of Ceph as workloads running within the cluster. This is the subject of a separate recipe and this recipe will focus on installing Ceph external to your k8s clsuter. For a more robust solution we will deploy a Ceph implementation outside of ICP and use that as the provider for a storage class which will allow any application to consume Ceph rbd dynamic storage. Doing so requires an external CEPH infrastructure. This document will walk through installing Ceph and integrating it with ICP for dynamic storage provisioning. Ceph Architecture We will use a distributed storage architecture. We will have three management nodes and three storage/compute nodes. Each storage/compute node has one disk with the operating system (/dev/sda) and two available raw disks for Ceph to consume (/dev/sdb, and /dev/sdc). Each node is connected to the network via two Mellanox ConnectX-4 cards configured for bonded 802.3ad link aggregation for 200Gb/s combined throughput. This provides for a hyperconverged and highly available architecture for both storage and data network traffic. OSD nodes do not host management functions and vice versa. The network architecture connecting these nodes is similar to that depicted in this diagram taken from the Cumulus documentation. All management nodes are redundant for high availability and management nodes should not run on the same physical nodes as storage nodes, so to implement this solution, you should use at least two servers (or VMs), one for hosting management nodes and one for hosting storage nodes. In a highly available environment, it is recommended to run three management nodes and at least three storage nodes so that it can handle the loss of any single node. In this document we will use ubuntu 18.04 as our host operating system. The hosts are named node1 - node6, respectively, and node1 will be our admin node. node2 and node3 will be additional management nodes for HA and node4 - node6 will be compute/storage nodes. We will first install Ceph and test creating and mounting a block device. Then we will integrate with ICP. Prepare for Ceph installation Download and install ceph-deploy IMPORTANT: Run the following commands as the root user Add the release Key wget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add - Add the CEPH packages to your repository echo deb https://download.ceph.com/debian-{ceph-stable-release}/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list replace {ceph-stable-release} with the release you would like to install e.g. mimic - debian-mimic Install ntp on all nodes apt-get install -y ntp If there is a local ntp server on your network, update /etc/ntp.conf with your local pool server and restart ntp. Install python on all nodes apt-get install -y python Update and install apt-get update apt-get install -y ceph-deploy Create a ceph-deploy user on all nodes. useradd -m -s /bin/bash -c \"ceph deploy user\" ceph-deploy echo \"ceph-deploy:Passw0rd!\" | sudo -S chpasswd Add ceph-deploy user to passwordless sudo on all nodes echo 'ceph-deploy ALL=(root) NOPASSWD:ALL' |sudo EDITOR='tee -a' visudo IMPORTANT: Run the following commands as the ceph-deploy user Enable running Ceph commands easier on other nodes Login as your ceph-deploy user and create the following file at ~/.ssh/config. You may need to create both the /home/ceph-deploy/.ssh directory and the config file. Host node1 Hostname node1 User ceph-deploy Host node2 Hostname node2 User ceph-deploy Host node3 Hostname node3 User ceph-deploy Host node4 Hostname node4 User ceph-deploy Host node5 Hostname node5 User ceph-deploy Host node6 Hostname node6 User ceph-deploy Enable passwordless SSH for the ceph-deploy user from the admin node to all other nodes. Execute these commands as the the ceph-deploy user. Accept all defaults. ssh-keygen -t rsa -P '' This will create ssh public and private keys in ~/.ssh . Copy the keys to all other nodes: ssh-copy-id -i ~/.ssh/id_rsa ceph-deploy@node1 ssh-copy-id -i ~/.ssh/id_rsa ceph-deploy@node2 ssh-copy-id -i ~/.ssh/id_rsa ceph-deploy@node3 ssh-copy-id -i ~/.ssh/id_rsa ceph-deploy@node4 ssh-copy-id -i ~/.ssh/id_rsa ceph-deploy@node5 ssh-copy-id -i ~/.ssh/id_rsa ceph-deploy@node6 It will ask you for the password for the ceph-deploy user, answer with the password you created when you created the user. When this is complete you should be able to execute ssh ceph-deploy@node1 and get from the ceph-deploy user on the admin host to the remote host without providing a password. IMPORTANT: Make sure you copy the ID back to the local node (node1) as well so the process can ssh back to itself. Deploy Ceph Execute the following commands as the ceph-deploy user on the admin node. Create the cluster From the ceph-deploy user's home directory: mkdir mycluster cd mycluster ceph-deploy new node1 Install Ceph on all nodes ceph-deploy install node1 node2 node3 node4 node5 node6 Deploy the initial monitor and gather the keys ceph-deploy mon create-initial Copy the admin config files to all nodes ceph-deploy admin node1 node2 node3 node4 node5 node6 Deploy a manager node ceph-deploy mgr create node1 Deploy storage nodes The data should be the raw device name of an unused raw device installed in the host. The final parameter is the hostname. Execute this command once for every raw device and host in the environment. ceph-deploy osd create --data /dev/sdb node4 ceph-deploy osd create --data /dev/sdc node4 ceph-deploy osd create --data /dev/sdb node5 ceph-deploy osd create --data /dev/sdc node5 ceph-deploy osd create --data /dev/sdb node6 ceph-deploy osd create --data /dev/sdc node6 Install a metadata server ceph-deploy mds create node1 Deploy the object gateway (S3/Swift) (optional) ceph-deploy rgw create node1 Deploy mgr to standby nodes for HA (optional) On the admin node edit /home/ceph-deploy/mycluster/ceph.conf file and update the mon_initial_members, mon_host, and public_network values to reflect the additional nodes. The resulting file should look something like this: [global] fsid = 264349d2-8eb0-4fb3-9992-bbef4c2759cc mon_initial_members = node1,node2,node3 mon_host = 10.10.2.1,10.10.2.2,10.10.2.3 public_network = 10.10.0.0/16 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx Then deploy the new nodes: ceph-deploy --overwrite-conf mon add node2 ceph-deploy --overwrite-conf mon add node3 Add additional mgr nodes for resiliency ceph-deploy --overwrite-conf mgr add node2 ceph-deploy --overwrite-conf mgr add node3 Check the status of your cluster sudo ceph -s The result should look something like this: cluster: id: 2fdde238-b426-4042-8cf3-6fc9a151cb9b health: HEALTH_OK services: mon: 3 daemons, quorum node1,node2,node3 mgr: node1(active), standbys: node2, node3 osd: 6 osds: 6 up, 6 in rgw: 1 daemon active data: pools: 4 pools, 1280 pgs objects: 221 objects, 1.2 KiB usage: 54 GiB used, 11 TiB / 11 TiB avail pgs: 1280 active+clean You should see HEALTH_OK. If not, look for your error message in the troubleshooting section below. If you did not install the rados gateway (rgw) then you will not yet see any pools defined. The likelihood is that your health message will say something like: health: HEALTH_WARN too few PGs per OSD (3 < min 30) If you do not see this error, you can skip this section until you do see it (and you will). A PG is a \"placement group\" and governs how data is stored in your environment. A full discussion of how this works is beyond the scope of this document, but resolving the warning can be done without knowing all of the details. For more information on this number see: http://docs.ceph.com/docs/giant/rados/operations/placement-groups/ https://stackoverflow.com/questions/39589696/ceph-too-many-pgs-per-osd-all-you-need-to-know There are two numbers that are important to modify to resolve this issue, the first is the PGs and the second is the PGPs. The PG is the number of placement groups available and the PGP is the number that are applied to your implementation. Any time you increase the PGs you should also increase the number of PGPs. The documentation recommends using PG numbers with powers of 2 (2, 4, 16, 32, 64, 128,...). The simple solution to this issue is to start with a smaller number, apply it and see what the status says. If it is still too small, continue to apply ever larger powers of 2 until the warning goes away. To change the number of PGs and PGPs, us the following command against every pool in your environment. To see the pools in your environment use the command: sudo ceph osd lspools Which should result in a list that looks something like this: 1 .rgw.root 2 default.rgw.control 3 default.rgw.meta 4 default.rgw.log For each pool in the list execute: sudo ceph osd pool set [pool name] pg_num <number> sudo ceph osd pool set [pool name] pgp_num <number> Example: sudo ceph osd pool set .rgw.root pg_num 32 sudo ceph osd pool set .rgw.root pgp_num 32 Then check your status and see if you need to raise it further. Continue increasing the number at the end of that command by powers of 2 until the warning goes away. Once you have a healthy cluster you can start using your new storage. The following command will show you all of your storage devices and their status. Note: OSD = Object Storage Daemon sudo ceph osd tree The result should look something like this: -1 192.85042 root default -3 87.32849 host node4 0 ssd 3.63869 osd.0 up 1.00000 1.00000 1 ssd 3.63869 osd.1 up 1.00000 1.00000 -5 47.30293 host node5 24 ssd 3.63869 osd.2 up 1.00000 1.00000 25 ssd 3.63869 osd.3 up 1.00000 1.00000 -7 14.55475 host node6 37 ssd 3.63869 osd.4 up 1.00000 1.00000 38 ssd 3.63869 osd.5 up 1.00000 1.00000 Test your newly installed Ceph instance Create and mount a block device Block devices are the most commonly used types of storage provisioned by Ceph users. Creating and using them is relatively easy once your environment is up and running. Block devices are known as rbd devices (Rados Block Device). When you create a new block device and attach it to your filesystem it will show up as /dev/rbd0, /eev/rbd1, etc. Before you can create a block device you need to create a new pool in which they can be stored. sudo ceph osd pool create rbd 128 128 NOTE: The two numbers at the end of this command are the PG and PGP for this pool. As a start, you should use the same values you used to get the health warning error to go away. These values may need to be changed based on the size of your environment and number of pools as per the above discussion. Once your pool has been created you can then create a new image in that pool. An image is block storage on which you can create a filesystem and is analogous to a virtual disk. sudo rbd create myimage --size 10240 --image-feature layering This command will create a new 10GB disk named \"myimage\" suitable for mounting on your filesystem. The --size parameter is in MB. To view the images in your pool use sudo rbd ls Now, ssh to the machine on which you want to mount this image. Before the storage can be mounted you must install the Ceph client on the target machine. sudo apt-get install -y ceph-common ceph-fuse Create yourself a mount point: sudo mkdir /mnt/myimage sudo rbd map myimage --name client.admin myimage is the name of the image you created previously everything else should be exactly as shown. The result of this command is a new device named /dev/rbd0. Next, put a filesystem on your new block device : sudo mkfs.ext4 -m0 /dev/rbd0 ... and mount your new filesystem at your created mount point: mount /dev/rbd0 /mnt/myimage Now, if you do an ls on your newly mounted filesystem you should see a lost+found directory indicating the root of a partition. Remove your test configuration Remove your test mount umount /mnt/myimage Remove the rbd image from your Ceph instance sudo rbd unmap myimage --name client.admin Remove the pool sudo ceph osd pool delete rbd Interating ICP with CEPH Create an rbd pool for use with ICP sudo ceph osd pool create icp rbd 1024 1024 Create a new ceph user for use with ICP sudo ceph auth get-or-create client.icp mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=icp' -o ceph.client.kube.keyring To deploy images as this user, you will need to create a keyring file for your worker nodes. sudo ceph auth get client.icp > ./ceph.client.icp.keyring Copy this keyring to /etc/ceph cp ./ceph.client.icp.keyring /etc/ceph/ Retrieve the Ceph admin key as base64 sudo ceph auth get-key client.admin |base64 This should return something like: QVFDSGhYZGIrcmc0SUJBQXd0Yy9pRXIxT1E1ZE5sMmdzRHhlZVE9PQ== Retrieve the Ceph ICP key as base64 sudo ceph auth get-key client.icp |base64 This should return something like: QVFERUlYNWJKbzlYR1JBQTRMVnU1N1YvWDhYbXAxc2tseDB6QkE9PQ== Before you can use ceph with ICP you must have the ceph-common package installed on each worker node apt-get install -y ceph-common You will now need to copy the ceph.client.admin.keyring and ceph.icp.admin.keyring to the /etc/ceph directory on each worker node scp root@cephnode:/etc/ceph/ceph.client.admin.keyring /etc/ceph/ scp root@cephnode:/etc/ceph/ceph.icp.admin.keyring /etc/ceph/ Note: If you do not have remote root login enabled (and you shouldn't) then you will need to copy these files to a temporary location which is accessible to a non-root user and then as root, ssh them to your worker nodes and move them to the /etc/ceph directory. Then delete them from the intermediate location. Create a new file named ceph-secret.yaml with the following contents: apiVersion: v1 kind: Secret metadata: name: ceph-secret namespace: kube-system data: key: QVFBOFF2SlZheUJQRVJBQWgvS2cwT1laQUhPQno3akZwekxxdGc9PQ== type: kubernetes.io/rbd Create the secret in ICP Use the ICP UI to configure your kubectl client and create the 'ceph-secret' secret with the following command: kubectl create -f ./ceph-secret.yaml Create a new file named ceph-user-secret.yaml with the following contents: apiVersion: v1 kind: Secret metadata: name: ceph-user-secret namespace: default data: key: QVFCbEV4OVpmaGJtQ0JBQW55d2Z0NHZtcS96cE42SW1JVUQvekE9PQ== type: kubernetes.io/rbd Where data.key is the key retrieved from ceph for the client.icp user. Create the user secret in ICP Use the ICP UI to configure your kubectl client and create the 'ceph-user-secret' secret in the default namespace with the following command: kubectl create -f ./ceph-user-secret.yaml Important Note: Because this user was created in the 'default' namespace (as noted in metadata.namespace above) This storage class can only be used in the default namespace. To use Ceph dynamic provisioning in other namespaces you must create the same user secret in every namespace where you want to deploy Ceph dynamic storage. Because the storage class specifically references \"ceph-user-secret\" the secret should always have this name no matter what namespace is used. Create the Ceph RBD Dynamic Storage Class Create a file named 'ceph-sc.yaml' with the following contents: apiVersion: storage.k8s.io/v1beta1 kind: StorageClass metadata: name: ceph annotations: storageclass.beta.kubernetes.io/is-default-class: \"true\" provisioner: kubernetes.io/rbd parameters: monitors: 10.10.0.1:6789,10.10.0.2:6789,10.10.0.3:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: icp userId: icp userSecretName: ceph-user-secret fsType: ext4 imageFormat: \"2\" Where parameters.monitors are the IP addresses and ports of all Ceph monitor nodes, comma separated. Remove metadata.annotations.storageclass.* if this should not be the default storage class. IMPORTANT: As noted above, there will need to be a secret named ceph-user-seret in each of the namespaces where will use Ceph dynamic storage provisioning. They should all be the same with the admin key for the userId user. Test your new storage class by creating a new PV from the ceph pool. Create a file named ceph-pvc.yaml with the following contents: kind: PersistentVolumeClaim apiVersion: v1 metadata: name: ceph-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi Create the PV with the following command: kubectl create -f ./ceph-pvc.yaml Check the status of your new PVC: kubectl get persistentvolumes root@master:/opt/icp/ceph# kubectl get persistentvolumes NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE helm-repo-pv 5Gi RWO Delete Bound kube-system/helm-repo-pvc helm-repo-storage 6d image-manager-10.10.10.1 20Gi RWO Retain Bound kube-system/image-manager-image-manager-0 image-manager-storage 6d logging-datanode-10.10.10.3 20Gi RWO Retain Bound kube-system/data-logging-elk-data-0 logging-storage-datanode 6d mongodb-10.10.10.1 20Gi RWO Retain Bound kube-system/mongodbdir-icp-mongodb-0 mongodb-storage 6d pvc-3b037115-a686-11e8-9387-5254006a2ffe 2Gi RWO Delete Bound default/ceph-pv-test ceph 1m Look for a PV with a storage class of \"ceph\" or In the ICP UI, navigate to Platform->Storage and look for a PV of type \"RBD\": List your created PVs from Ceph: On your ceph admin or monitor node execute: sudo rbd list You should see something like: $ sudo rbd list kubernetes-dynamic-pvc-7d5c8c11-a687-11e8-9291-5254006a2ffe Remove your test PVC with the following command: kubectl delete -f ./ceph-pvc.yaml To use your storage class with a deployment you must install the Ceph client on all schedulable nodes . Execute the following on all ICP worker nodes: apt-get install -y ceph-common Copy /etc/ceph/ceph.conf, /etc/ceph/ceph.client.icp.keyring, and /etc/ceph/ceph.client.admin.keyring from your Ceph admin node to each worker node. From each worker node as root execute: scp root@ceph-admin:/etc/ceph/ceph.conf /etc/ceph/ scp root@ceph-admin:/etc/ceph/*.keyring /etc/ceph/ or Copy these files to the boot node and use scp to move them to all worker nodes without having to login once for each worker node (assuming you have configured passwordless ssh from your boot node to all your worker nodes) Depending on the version of Ceph and kubernetes you are using you may get an error when attempting to deploy a pod using Ceph dynamic storage. The event will look something like this: MountVolume.WaitForAttach failed for volume \"pvc-f00434db-a8d6-11e8-9387-5254006a2ffe\" : rbd: map failed exit status 110, rbd output: rbd: sysfs write failed In some cases useful info is found in syslog - try \"dmesg | tail\" or so. rbd: map failed: (110) Connection timed out Further investigation into the syslog file on the worker node should show an entry something like this: Aug 25 13:32:50 worker1 kernel: [745415.055916] libceph: mon0 10.10.2.1:6789 feature set mismatch, my 106b84a842a42 < server's 40106b84a842a42, missing 400000000000000 ... along with a bunch of other error messages This error message indicates a missing feature flag in the Ceph client. The feature missing is CRUSH_TUNABLES5 To resolve this issue execute the following command on your Ceph admin or monitor node: sudo ceph osd crush tunables hammer Your pod should now finish provisioning. Recipe 3: Deploying an Internal Ceph/Rook Instance Recipe 4: Deploying an Internal GlusterFS Instance Recipe 5: Deploying an External GlusterFS Instance Recipe 6: Integrating with an existing VMware vCenter 6.5 Instance It is important to note that using VMware storage for dynamic storage provisioning uses the VMware API and this is not vSAN. vSAN is a completely separate technology which takes storage local to multiple hypervisors and makes it available as a VMware datastore. To use VMware for dynamic storage provisioning you need an existing VMware datastore and access to the API server with the needed credentials for creating storage volumes. That is what this tutorial will focus on. All of the prerequisites noted here must be complete prior to installing ICP. Configure vSphere for use by ICP First, to use dynamic storage provisioning on VMware you must create a user with the proper VMware permissions that can be used to interact with VMware. The IBM knowledgebase for this topic can be found at https://www.ibm.com/support/knowledgecenter/en/SSBS6K_2.1.0.3/manage_cluster/add_vsphere.html. Prerequisites for creating the vssphere storage can be found at https://www.ibm.com/support/knowledgecenter/SSBS6K_2.1.0.3/installing/cloud_provider_vsphere.html#prereq It should be noted that as of this writing, only the ReadWriteOnce storage access mode. The following are important restrictions on using VMware for your dynamic storage. All IBM\u00ae Cloud Private cluster nodes must be under one vSphere VM folder. All IBM Cloud Private master nodes must be able to access vCenter. The node host name must be same as the VM name. Node host names must comply with the regex: [ a-z] ?(\\.[ a-z0-9] ?)*, and must also comply with the following restrictions: They must not begin with numbers. They must not use capital letters. They must not have any special characters except . and -. They must contain at least three characters but no more than 63 characters. The disk UUID on the node VMs must be enabled: the disk.EnableUUID value must be set to True. The user that is specified in the vSphere cloud configuration must have privileges to interact with vCenter. You will ned to create a vCenter user to use for the dynamic storage provisioning. We will use \"icpadmin\" as our user. We also need to create a few roles and assign them to this user so that it has the correct authority. The following information is correct for VMware 6.5 and may not be applicable for other versions. Role 1: manage-k8s-node-vms Privileges: * Resource: Assign virtual machine to resource pool * Virtual Machine -> Configuration: Add existing disk * Virtual Machine -> Configuration: Add new disk * Virtual Machine -> Configuration: Add or remove device * Virtual Machine -> Configuration: Remove disk * Virtual Machine -> Inventory: Create from existing * Virtual Machine -> Inventory: Create new * Virtual Machine -> Inventory: Create remove Role 2: manage-k8s-volumes Privileges: * Datastore -> Allocate space * Datastore -> Browse datastore * Datastore -> Configure datastore * Datastore -> Low level file operations * Datastore -> Remove file * Datastore -> Update virtual machine files * Datastore -> Update virtual machine metadata Role 3: k8s-system-read-and-spbm-profile-view Privileges: * Storage views: Configure service * Storage views: view Role 4: ReadOnly Privileges: none Alternatively, you can just create a single role called \"icpadmin\" which has all of these privileges. Next, assign the following roles for this user on all the needed vmware objects: * Datacenter (only): k8s-system-read-and-spbm-profile-view * Cluster (propogate ): manage-k8s-node-vms * VM Folder (propogate): manage-k8s-node-vms * Target Datastore (only): manage-k8s-volumes * Datacenter, Datastore Cluster, and Datastore storage folder: ReadOnly If you have created one role with all needed privileges, just assign that role to that user for all of the entities noted above: All pertinent Datacenters, clusters, hosts, resource pools, datastores, and folders. Configure ICP for vSphere storage On your ICP boot node, open the config.yaml file and add the following (spacing is important, use spaces and not tabs): kubelet_nodename: hostname cloud_provider: vsphere vsphere_conf: user: \"<vCenter username for vSphere Cloud Provider>\" password: \"<password for vCenter user>\" server: <vCenter server IP or FQDN> port: [vCenter Server Port; default: 443] insecure_flag: [set to 1 if vCenter uses a self-signed certificate] datacenter: <datacenter name on which Node VMs are deployed> datastore: <default datastore to be used for provisioning volumes> working_dir: <vCenter VM folder path in which node VMs are located> Example: kubelet_nodename: hostname cloud_provider: vsphere vsphere_conf: user: icpadmin password: Passw0rd! server: 1.2.3.4 port: 443 insecure_flag: 1 datacenter: CSPLAB datastore: ExternalDemo working_dir: my-icp-2103 Deploy your ICP instance as per normal. Once your instance has been deployed you must create a storage class to consume the storage. First, Create a .yaml file (vsphere.yaml) with the following contents: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: vsphere annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: kubernetes.io/vsphere-volume parameters: diskformat: thin datastore: MyDatastore The diskformat can be thin,zeroedthick, or eagerzeroedthick. The datastore should be changed to specify the name of the datastore where you want your volumes created. Configure kubectl to point to your ICP instance. Deploy your new storage class kubectl create -f vsphere.yaml Now, when installing helm charts you can specify dynamic storage using the 'vsphere' storage class and it will dynamically provision your PV to the specified datastore. Recipe 7: Integrating with an existing S3 Storage Provider","title":"Part II - Kubernetes Storage Recipes"},{"location":"recipes/#table-of-contents","text":"Introduction Things to Consider Prior to Choosing a Storage Solution Recipes Integrating with an External NFS Server Deploying an External Ceph Instance and Integrating with ICP Deploying an Internal Ceph/Rook Instance Deploying an External Gluster Instance Deploying an Internal Gluster Instance Integrating with an existing VMware vCenter 6.5 Instance Integrating with an existing S3 storage provider","title":"Table of Contents"},{"location":"recipes/#introduction","text":"Kubernetes can consume storage solutions deployed either as a part of a cluster (internal storage) or storage provided by an external service (external storage). Deploying a workload storage solution as a part of a cluster (internal storage) will limit access to the storage to workloads running inside the cluster. This will be more secure than deploying an external instance of the storage provider, but also limits who can consume the storage unless steps are taken to expose the storage outside the cluster. The use case will determine which you choose to deploy. If your objective is to have a large storage solution which can be consumed by multiple kubernetes clusters or non-kubernetes workloads, then your solution should include an instance of the storage provider which is external to the k8s cluster and then integrate your k8s instance with that storage provider via storage classes. If your objective is to have a secure storage solution which is only supporting a single k8s cluster, then you will want to deploy a storage provider inside the cluster and point your storage classes to the internal instance. Another alternative is to have a single k8s cluster which implements an internal storage provider but exposes the API service outside the cluster. This could then be used as a storage provider for other k8s clusters. It should be noted that the recipes in this document will describe implementing block and filesystem storage for workloads only (not platform storage) and, other than S3 storage providers, will not consider object storage.","title":"Introduction"},{"location":"recipes/#things-to-consider-prior-to-choosing-a-storage-solution","text":"As with all cloud resources, high availability is of primary concern. When creating a storage provider either internally or externally, the instance should be architected such that an entire node can fail and the storage provider can survive the failure. k8s is designed to be highly elastic and fungible. Nodes and processes can fail and k8s will just redeploy them elsewhere assuming adequate available resources. Consider a situation where an internal GlusterFS instance has been created in a smaller k8s environment and three storage nodes are provisioned. If the utilization of the overall storage environment reaches is high and the implementation of the storage provider does not have enough resources to spread the load well, data could be lost and/or the storage provider could go down altogether. Just like master nodes, storage nodes should run with anti-affinity rules such that no two storage nodes are running on the same physical infrastructure. This way the loss of a single physical node does not take out 2/3 of your available capacity. As a rule, a larger volume of storage nodes spread accross a larger number of physical nodes will be a better solution with a larger tolerance for failure. One should also avoid running multiple Ceph OSDs or Gluster Bricks on the same node. It does no good to have 9 GlusterFS bricks in your environment if they are running on only three nodes and one of the nodes fails. In this case, you have still lost 30% of your capacity with one failure. Different storage technologies can handle various amounts of loss (see product documentation for specific details for your chosen technology). A solution architect should be aware of those details and ensure whatever solution they choose to implement can handle the loss of at least one node. The number of nodes you choose to tolerate may be more than one based on business need, but any solution should take advantage of the traits of the k8s platform and ensure the storage infrastructure is properly architected.","title":"Things to Consider Prior to Choosing a Storage Solution"},{"location":"recipes/#recipes","text":"","title":"Recipes"},{"location":"recipes/#recipe-1-integrating-with-an-external-nfs-server","text":"IMPORTANT: Whereas NFS is probably the easiest storage provider to configure for k8s and is useful in a development environment, IBM does not recommend using NFS for workload storage in production for a number of reasons. The standard NFS storage provider for k8s is not dynamic and requires NFS PVs be manually created (or scripted) and manual processes are prone to errors and can cause a delay for end users if no PVs exist with the needed attributes. This requires an operator to manually intervene in the DevOps process which is against basic DevOps principles. NFS is not secure. NFS is slow. If you need an NFS storage provider for dev purposes or it is required for some other reason, the following will help you make sure it is effective in a kubernetes environment. Creating the NFS Server Instructions for installing an NFS server differ slightly based on the operating system and instructions for doing so are ubiquitous on the internet. It is a fairly trivial exercise and will not be reproduced here. There are, however, a few recommendations for how to configure it once it is installed. Recommendations for the NFS server: Use LVM device mapping so that additional storage space can be added if needed. Do not give global mount permissions for NFS paths. This could allow different users on different clusters to mount the same path and overwrite each other. Each k8s cluster should have its own mount path. If sharing the NFS server with other non-k8s services, each other server should have its own mount permissions and mount path with no globol mount permissions. If a user can mount an NFS path via k8s and set the PV attribute to \"reclaim\", when the workload is removed k8s will run rm -rf / at the mount path which will result in permanently deleting anything on that path. Extreme caution should be exercised to prevent this possibility. Make sure attributes on the mount path include \"sync\" to prevent sync errors when consuming ReadWriteMany (RWX) PVs. admin@nfs-server:~$ cat /etc/exports /storage/cluster1 1.2.3.4(rw,no_subtree_check,sync,insecure,no_root_squash) Consuming NFS storage Create a new .yaml file with contents similar to the following: # myNfsVolume1.yaml kind: PersistentVolume apiVersion: v1 metadata: name: myNfsVolume1 spec: capacity: storage: 5Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle nfs: path: /storage/cluster1/myNfsVolume1 server: 1.2.3.4 This PV will have a total of 5GB of storage space, allow only one pod to have access, and when the deployment that is consuming it is deleted, its contents will be wiped and it will be placed back in the pool. Install your new PV using kubectl: kubectl create -f myNfsVolume1.yaml","title":"Recipe 1: Integrating with an External NFS server"},{"location":"recipes/#recipe-2-deploying-an-external-ceph-instance-and-integrating-with-icp","text":"Ceph is short for \"cephalopod\", a class of mollusks of which the octopus is a member. The octopus is used as the logo for Ceph and this name was chosen because the parallel processing nature of both the octopus and the software. There are two ways of integrating ICP with Ceph. The Ceph Rook open source project provides support for running the entire Ceph environment within a k8s cluster, utilizing local (hostpath) storage as raw disk for the OSD nodes. This method will install all of Ceph as workloads running within the cluster. This is the subject of a separate recipe and this recipe will focus on installing Ceph external to your k8s clsuter. For a more robust solution we will deploy a Ceph implementation outside of ICP and use that as the provider for a storage class which will allow any application to consume Ceph rbd dynamic storage. Doing so requires an external CEPH infrastructure. This document will walk through installing Ceph and integrating it with ICP for dynamic storage provisioning. Ceph Architecture We will use a distributed storage architecture. We will have three management nodes and three storage/compute nodes. Each storage/compute node has one disk with the operating system (/dev/sda) and two available raw disks for Ceph to consume (/dev/sdb, and /dev/sdc). Each node is connected to the network via two Mellanox ConnectX-4 cards configured for bonded 802.3ad link aggregation for 200Gb/s combined throughput. This provides for a hyperconverged and highly available architecture for both storage and data network traffic. OSD nodes do not host management functions and vice versa. The network architecture connecting these nodes is similar to that depicted in this diagram taken from the Cumulus documentation. All management nodes are redundant for high availability and management nodes should not run on the same physical nodes as storage nodes, so to implement this solution, you should use at least two servers (or VMs), one for hosting management nodes and one for hosting storage nodes. In a highly available environment, it is recommended to run three management nodes and at least three storage nodes so that it can handle the loss of any single node. In this document we will use ubuntu 18.04 as our host operating system. The hosts are named node1 - node6, respectively, and node1 will be our admin node. node2 and node3 will be additional management nodes for HA and node4 - node6 will be compute/storage nodes. We will first install Ceph and test creating and mounting a block device. Then we will integrate with ICP. Prepare for Ceph installation Download and install ceph-deploy IMPORTANT: Run the following commands as the root user Add the release Key wget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add - Add the CEPH packages to your repository echo deb https://download.ceph.com/debian-{ceph-stable-release}/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list replace {ceph-stable-release} with the release you would like to install e.g. mimic - debian-mimic Install ntp on all nodes apt-get install -y ntp If there is a local ntp server on your network, update /etc/ntp.conf with your local pool server and restart ntp. Install python on all nodes apt-get install -y python Update and install apt-get update apt-get install -y ceph-deploy Create a ceph-deploy user on all nodes. useradd -m -s /bin/bash -c \"ceph deploy user\" ceph-deploy echo \"ceph-deploy:Passw0rd!\" | sudo -S chpasswd Add ceph-deploy user to passwordless sudo on all nodes echo 'ceph-deploy ALL=(root) NOPASSWD:ALL' |sudo EDITOR='tee -a' visudo IMPORTANT: Run the following commands as the ceph-deploy user Enable running Ceph commands easier on other nodes Login as your ceph-deploy user and create the following file at ~/.ssh/config. You may need to create both the /home/ceph-deploy/.ssh directory and the config file. Host node1 Hostname node1 User ceph-deploy Host node2 Hostname node2 User ceph-deploy Host node3 Hostname node3 User ceph-deploy Host node4 Hostname node4 User ceph-deploy Host node5 Hostname node5 User ceph-deploy Host node6 Hostname node6 User ceph-deploy Enable passwordless SSH for the ceph-deploy user from the admin node to all other nodes. Execute these commands as the the ceph-deploy user. Accept all defaults. ssh-keygen -t rsa -P '' This will create ssh public and private keys in ~/.ssh . Copy the keys to all other nodes: ssh-copy-id -i ~/.ssh/id_rsa ceph-deploy@node1 ssh-copy-id -i ~/.ssh/id_rsa ceph-deploy@node2 ssh-copy-id -i ~/.ssh/id_rsa ceph-deploy@node3 ssh-copy-id -i ~/.ssh/id_rsa ceph-deploy@node4 ssh-copy-id -i ~/.ssh/id_rsa ceph-deploy@node5 ssh-copy-id -i ~/.ssh/id_rsa ceph-deploy@node6 It will ask you for the password for the ceph-deploy user, answer with the password you created when you created the user. When this is complete you should be able to execute ssh ceph-deploy@node1 and get from the ceph-deploy user on the admin host to the remote host without providing a password. IMPORTANT: Make sure you copy the ID back to the local node (node1) as well so the process can ssh back to itself. Deploy Ceph Execute the following commands as the ceph-deploy user on the admin node. Create the cluster From the ceph-deploy user's home directory: mkdir mycluster cd mycluster ceph-deploy new node1 Install Ceph on all nodes ceph-deploy install node1 node2 node3 node4 node5 node6 Deploy the initial monitor and gather the keys ceph-deploy mon create-initial Copy the admin config files to all nodes ceph-deploy admin node1 node2 node3 node4 node5 node6 Deploy a manager node ceph-deploy mgr create node1 Deploy storage nodes The data should be the raw device name of an unused raw device installed in the host. The final parameter is the hostname. Execute this command once for every raw device and host in the environment. ceph-deploy osd create --data /dev/sdb node4 ceph-deploy osd create --data /dev/sdc node4 ceph-deploy osd create --data /dev/sdb node5 ceph-deploy osd create --data /dev/sdc node5 ceph-deploy osd create --data /dev/sdb node6 ceph-deploy osd create --data /dev/sdc node6 Install a metadata server ceph-deploy mds create node1 Deploy the object gateway (S3/Swift) (optional) ceph-deploy rgw create node1 Deploy mgr to standby nodes for HA (optional) On the admin node edit /home/ceph-deploy/mycluster/ceph.conf file and update the mon_initial_members, mon_host, and public_network values to reflect the additional nodes. The resulting file should look something like this: [global] fsid = 264349d2-8eb0-4fb3-9992-bbef4c2759cc mon_initial_members = node1,node2,node3 mon_host = 10.10.2.1,10.10.2.2,10.10.2.3 public_network = 10.10.0.0/16 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx Then deploy the new nodes: ceph-deploy --overwrite-conf mon add node2 ceph-deploy --overwrite-conf mon add node3 Add additional mgr nodes for resiliency ceph-deploy --overwrite-conf mgr add node2 ceph-deploy --overwrite-conf mgr add node3 Check the status of your cluster sudo ceph -s The result should look something like this: cluster: id: 2fdde238-b426-4042-8cf3-6fc9a151cb9b health: HEALTH_OK services: mon: 3 daemons, quorum node1,node2,node3 mgr: node1(active), standbys: node2, node3 osd: 6 osds: 6 up, 6 in rgw: 1 daemon active data: pools: 4 pools, 1280 pgs objects: 221 objects, 1.2 KiB usage: 54 GiB used, 11 TiB / 11 TiB avail pgs: 1280 active+clean You should see HEALTH_OK. If not, look for your error message in the troubleshooting section below. If you did not install the rados gateway (rgw) then you will not yet see any pools defined. The likelihood is that your health message will say something like: health: HEALTH_WARN too few PGs per OSD (3 < min 30) If you do not see this error, you can skip this section until you do see it (and you will). A PG is a \"placement group\" and governs how data is stored in your environment. A full discussion of how this works is beyond the scope of this document, but resolving the warning can be done without knowing all of the details. For more information on this number see: http://docs.ceph.com/docs/giant/rados/operations/placement-groups/ https://stackoverflow.com/questions/39589696/ceph-too-many-pgs-per-osd-all-you-need-to-know There are two numbers that are important to modify to resolve this issue, the first is the PGs and the second is the PGPs. The PG is the number of placement groups available and the PGP is the number that are applied to your implementation. Any time you increase the PGs you should also increase the number of PGPs. The documentation recommends using PG numbers with powers of 2 (2, 4, 16, 32, 64, 128,...). The simple solution to this issue is to start with a smaller number, apply it and see what the status says. If it is still too small, continue to apply ever larger powers of 2 until the warning goes away. To change the number of PGs and PGPs, us the following command against every pool in your environment. To see the pools in your environment use the command: sudo ceph osd lspools Which should result in a list that looks something like this: 1 .rgw.root 2 default.rgw.control 3 default.rgw.meta 4 default.rgw.log For each pool in the list execute: sudo ceph osd pool set [pool name] pg_num <number> sudo ceph osd pool set [pool name] pgp_num <number> Example: sudo ceph osd pool set .rgw.root pg_num 32 sudo ceph osd pool set .rgw.root pgp_num 32 Then check your status and see if you need to raise it further. Continue increasing the number at the end of that command by powers of 2 until the warning goes away. Once you have a healthy cluster you can start using your new storage. The following command will show you all of your storage devices and their status. Note: OSD = Object Storage Daemon sudo ceph osd tree The result should look something like this: -1 192.85042 root default -3 87.32849 host node4 0 ssd 3.63869 osd.0 up 1.00000 1.00000 1 ssd 3.63869 osd.1 up 1.00000 1.00000 -5 47.30293 host node5 24 ssd 3.63869 osd.2 up 1.00000 1.00000 25 ssd 3.63869 osd.3 up 1.00000 1.00000 -7 14.55475 host node6 37 ssd 3.63869 osd.4 up 1.00000 1.00000 38 ssd 3.63869 osd.5 up 1.00000 1.00000 Test your newly installed Ceph instance Create and mount a block device Block devices are the most commonly used types of storage provisioned by Ceph users. Creating and using them is relatively easy once your environment is up and running. Block devices are known as rbd devices (Rados Block Device). When you create a new block device and attach it to your filesystem it will show up as /dev/rbd0, /eev/rbd1, etc. Before you can create a block device you need to create a new pool in which they can be stored. sudo ceph osd pool create rbd 128 128 NOTE: The two numbers at the end of this command are the PG and PGP for this pool. As a start, you should use the same values you used to get the health warning error to go away. These values may need to be changed based on the size of your environment and number of pools as per the above discussion. Once your pool has been created you can then create a new image in that pool. An image is block storage on which you can create a filesystem and is analogous to a virtual disk. sudo rbd create myimage --size 10240 --image-feature layering This command will create a new 10GB disk named \"myimage\" suitable for mounting on your filesystem. The --size parameter is in MB. To view the images in your pool use sudo rbd ls Now, ssh to the machine on which you want to mount this image. Before the storage can be mounted you must install the Ceph client on the target machine. sudo apt-get install -y ceph-common ceph-fuse Create yourself a mount point: sudo mkdir /mnt/myimage sudo rbd map myimage --name client.admin myimage is the name of the image you created previously everything else should be exactly as shown. The result of this command is a new device named /dev/rbd0. Next, put a filesystem on your new block device : sudo mkfs.ext4 -m0 /dev/rbd0 ... and mount your new filesystem at your created mount point: mount /dev/rbd0 /mnt/myimage Now, if you do an ls on your newly mounted filesystem you should see a lost+found directory indicating the root of a partition. Remove your test configuration Remove your test mount umount /mnt/myimage Remove the rbd image from your Ceph instance sudo rbd unmap myimage --name client.admin Remove the pool sudo ceph osd pool delete rbd Interating ICP with CEPH Create an rbd pool for use with ICP sudo ceph osd pool create icp rbd 1024 1024 Create a new ceph user for use with ICP sudo ceph auth get-or-create client.icp mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=icp' -o ceph.client.kube.keyring To deploy images as this user, you will need to create a keyring file for your worker nodes. sudo ceph auth get client.icp > ./ceph.client.icp.keyring Copy this keyring to /etc/ceph cp ./ceph.client.icp.keyring /etc/ceph/ Retrieve the Ceph admin key as base64 sudo ceph auth get-key client.admin |base64 This should return something like: QVFDSGhYZGIrcmc0SUJBQXd0Yy9pRXIxT1E1ZE5sMmdzRHhlZVE9PQ== Retrieve the Ceph ICP key as base64 sudo ceph auth get-key client.icp |base64 This should return something like: QVFERUlYNWJKbzlYR1JBQTRMVnU1N1YvWDhYbXAxc2tseDB6QkE9PQ== Before you can use ceph with ICP you must have the ceph-common package installed on each worker node apt-get install -y ceph-common You will now need to copy the ceph.client.admin.keyring and ceph.icp.admin.keyring to the /etc/ceph directory on each worker node scp root@cephnode:/etc/ceph/ceph.client.admin.keyring /etc/ceph/ scp root@cephnode:/etc/ceph/ceph.icp.admin.keyring /etc/ceph/ Note: If you do not have remote root login enabled (and you shouldn't) then you will need to copy these files to a temporary location which is accessible to a non-root user and then as root, ssh them to your worker nodes and move them to the /etc/ceph directory. Then delete them from the intermediate location. Create a new file named ceph-secret.yaml with the following contents: apiVersion: v1 kind: Secret metadata: name: ceph-secret namespace: kube-system data: key: QVFBOFF2SlZheUJQRVJBQWgvS2cwT1laQUhPQno3akZwekxxdGc9PQ== type: kubernetes.io/rbd Create the secret in ICP Use the ICP UI to configure your kubectl client and create the 'ceph-secret' secret with the following command: kubectl create -f ./ceph-secret.yaml Create a new file named ceph-user-secret.yaml with the following contents: apiVersion: v1 kind: Secret metadata: name: ceph-user-secret namespace: default data: key: QVFCbEV4OVpmaGJtQ0JBQW55d2Z0NHZtcS96cE42SW1JVUQvekE9PQ== type: kubernetes.io/rbd Where data.key is the key retrieved from ceph for the client.icp user. Create the user secret in ICP Use the ICP UI to configure your kubectl client and create the 'ceph-user-secret' secret in the default namespace with the following command: kubectl create -f ./ceph-user-secret.yaml Important Note: Because this user was created in the 'default' namespace (as noted in metadata.namespace above) This storage class can only be used in the default namespace. To use Ceph dynamic provisioning in other namespaces you must create the same user secret in every namespace where you want to deploy Ceph dynamic storage. Because the storage class specifically references \"ceph-user-secret\" the secret should always have this name no matter what namespace is used. Create the Ceph RBD Dynamic Storage Class Create a file named 'ceph-sc.yaml' with the following contents: apiVersion: storage.k8s.io/v1beta1 kind: StorageClass metadata: name: ceph annotations: storageclass.beta.kubernetes.io/is-default-class: \"true\" provisioner: kubernetes.io/rbd parameters: monitors: 10.10.0.1:6789,10.10.0.2:6789,10.10.0.3:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: icp userId: icp userSecretName: ceph-user-secret fsType: ext4 imageFormat: \"2\" Where parameters.monitors are the IP addresses and ports of all Ceph monitor nodes, comma separated. Remove metadata.annotations.storageclass.* if this should not be the default storage class. IMPORTANT: As noted above, there will need to be a secret named ceph-user-seret in each of the namespaces where will use Ceph dynamic storage provisioning. They should all be the same with the admin key for the userId user. Test your new storage class by creating a new PV from the ceph pool. Create a file named ceph-pvc.yaml with the following contents: kind: PersistentVolumeClaim apiVersion: v1 metadata: name: ceph-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi Create the PV with the following command: kubectl create -f ./ceph-pvc.yaml Check the status of your new PVC: kubectl get persistentvolumes root@master:/opt/icp/ceph# kubectl get persistentvolumes NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE helm-repo-pv 5Gi RWO Delete Bound kube-system/helm-repo-pvc helm-repo-storage 6d image-manager-10.10.10.1 20Gi RWO Retain Bound kube-system/image-manager-image-manager-0 image-manager-storage 6d logging-datanode-10.10.10.3 20Gi RWO Retain Bound kube-system/data-logging-elk-data-0 logging-storage-datanode 6d mongodb-10.10.10.1 20Gi RWO Retain Bound kube-system/mongodbdir-icp-mongodb-0 mongodb-storage 6d pvc-3b037115-a686-11e8-9387-5254006a2ffe 2Gi RWO Delete Bound default/ceph-pv-test ceph 1m Look for a PV with a storage class of \"ceph\" or In the ICP UI, navigate to Platform->Storage and look for a PV of type \"RBD\": List your created PVs from Ceph: On your ceph admin or monitor node execute: sudo rbd list You should see something like: $ sudo rbd list kubernetes-dynamic-pvc-7d5c8c11-a687-11e8-9291-5254006a2ffe Remove your test PVC with the following command: kubectl delete -f ./ceph-pvc.yaml To use your storage class with a deployment you must install the Ceph client on all schedulable nodes . Execute the following on all ICP worker nodes: apt-get install -y ceph-common Copy /etc/ceph/ceph.conf, /etc/ceph/ceph.client.icp.keyring, and /etc/ceph/ceph.client.admin.keyring from your Ceph admin node to each worker node. From each worker node as root execute: scp root@ceph-admin:/etc/ceph/ceph.conf /etc/ceph/ scp root@ceph-admin:/etc/ceph/*.keyring /etc/ceph/ or Copy these files to the boot node and use scp to move them to all worker nodes without having to login once for each worker node (assuming you have configured passwordless ssh from your boot node to all your worker nodes) Depending on the version of Ceph and kubernetes you are using you may get an error when attempting to deploy a pod using Ceph dynamic storage. The event will look something like this: MountVolume.WaitForAttach failed for volume \"pvc-f00434db-a8d6-11e8-9387-5254006a2ffe\" : rbd: map failed exit status 110, rbd output: rbd: sysfs write failed In some cases useful info is found in syslog - try \"dmesg | tail\" or so. rbd: map failed: (110) Connection timed out Further investigation into the syslog file on the worker node should show an entry something like this: Aug 25 13:32:50 worker1 kernel: [745415.055916] libceph: mon0 10.10.2.1:6789 feature set mismatch, my 106b84a842a42 < server's 40106b84a842a42, missing 400000000000000 ... along with a bunch of other error messages This error message indicates a missing feature flag in the Ceph client. The feature missing is CRUSH_TUNABLES5 To resolve this issue execute the following command on your Ceph admin or monitor node: sudo ceph osd crush tunables hammer Your pod should now finish provisioning.","title":"Recipe 2: Deploying an External Ceph Instance and Integrating with ICP"},{"location":"recipes/#recipe-3-deploying-an-internal-cephrook-instance","text":"","title":"Recipe 3: Deploying an Internal Ceph/Rook Instance"},{"location":"recipes/#recipe-4-deploying-an-internal-glusterfs-instance","text":"","title":"Recipe 4: Deploying an Internal GlusterFS Instance"},{"location":"recipes/#recipe-5-deploying-an-external-glusterfs-instance","text":"","title":"Recipe 5: Deploying an External GlusterFS Instance"},{"location":"recipes/#recipe-6-integrating-with-an-existing-vmware-vcenter-65-instance","text":"It is important to note that using VMware storage for dynamic storage provisioning uses the VMware API and this is not vSAN. vSAN is a completely separate technology which takes storage local to multiple hypervisors and makes it available as a VMware datastore. To use VMware for dynamic storage provisioning you need an existing VMware datastore and access to the API server with the needed credentials for creating storage volumes. That is what this tutorial will focus on. All of the prerequisites noted here must be complete prior to installing ICP.","title":"Recipe 6: Integrating with an existing VMware vCenter 6.5 Instance"},{"location":"recipes/#configure-vsphere-for-use-by-icp","text":"First, to use dynamic storage provisioning on VMware you must create a user with the proper VMware permissions that can be used to interact with VMware. The IBM knowledgebase for this topic can be found at https://www.ibm.com/support/knowledgecenter/en/SSBS6K_2.1.0.3/manage_cluster/add_vsphere.html. Prerequisites for creating the vssphere storage can be found at https://www.ibm.com/support/knowledgecenter/SSBS6K_2.1.0.3/installing/cloud_provider_vsphere.html#prereq It should be noted that as of this writing, only the ReadWriteOnce storage access mode. The following are important restrictions on using VMware for your dynamic storage. All IBM\u00ae Cloud Private cluster nodes must be under one vSphere VM folder. All IBM Cloud Private master nodes must be able to access vCenter. The node host name must be same as the VM name. Node host names must comply with the regex: [ a-z] ?(\\.[ a-z0-9] ?)*, and must also comply with the following restrictions: They must not begin with numbers. They must not use capital letters. They must not have any special characters except . and -. They must contain at least three characters but no more than 63 characters. The disk UUID on the node VMs must be enabled: the disk.EnableUUID value must be set to True. The user that is specified in the vSphere cloud configuration must have privileges to interact with vCenter. You will ned to create a vCenter user to use for the dynamic storage provisioning. We will use \"icpadmin\" as our user. We also need to create a few roles and assign them to this user so that it has the correct authority. The following information is correct for VMware 6.5 and may not be applicable for other versions. Role 1: manage-k8s-node-vms Privileges: * Resource: Assign virtual machine to resource pool * Virtual Machine -> Configuration: Add existing disk * Virtual Machine -> Configuration: Add new disk * Virtual Machine -> Configuration: Add or remove device * Virtual Machine -> Configuration: Remove disk * Virtual Machine -> Inventory: Create from existing * Virtual Machine -> Inventory: Create new * Virtual Machine -> Inventory: Create remove Role 2: manage-k8s-volumes Privileges: * Datastore -> Allocate space * Datastore -> Browse datastore * Datastore -> Configure datastore * Datastore -> Low level file operations * Datastore -> Remove file * Datastore -> Update virtual machine files * Datastore -> Update virtual machine metadata Role 3: k8s-system-read-and-spbm-profile-view Privileges: * Storage views: Configure service * Storage views: view Role 4: ReadOnly Privileges: none Alternatively, you can just create a single role called \"icpadmin\" which has all of these privileges. Next, assign the following roles for this user on all the needed vmware objects: * Datacenter (only): k8s-system-read-and-spbm-profile-view * Cluster (propogate ): manage-k8s-node-vms * VM Folder (propogate): manage-k8s-node-vms * Target Datastore (only): manage-k8s-volumes * Datacenter, Datastore Cluster, and Datastore storage folder: ReadOnly If you have created one role with all needed privileges, just assign that role to that user for all of the entities noted above: All pertinent Datacenters, clusters, hosts, resource pools, datastores, and folders. Configure ICP for vSphere storage On your ICP boot node, open the config.yaml file and add the following (spacing is important, use spaces and not tabs): kubelet_nodename: hostname cloud_provider: vsphere vsphere_conf: user: \"<vCenter username for vSphere Cloud Provider>\" password: \"<password for vCenter user>\" server: <vCenter server IP or FQDN> port: [vCenter Server Port; default: 443] insecure_flag: [set to 1 if vCenter uses a self-signed certificate] datacenter: <datacenter name on which Node VMs are deployed> datastore: <default datastore to be used for provisioning volumes> working_dir: <vCenter VM folder path in which node VMs are located> Example: kubelet_nodename: hostname cloud_provider: vsphere vsphere_conf: user: icpadmin password: Passw0rd! server: 1.2.3.4 port: 443 insecure_flag: 1 datacenter: CSPLAB datastore: ExternalDemo working_dir: my-icp-2103 Deploy your ICP instance as per normal. Once your instance has been deployed you must create a storage class to consume the storage. First, Create a .yaml file (vsphere.yaml) with the following contents: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: vsphere annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: kubernetes.io/vsphere-volume parameters: diskformat: thin datastore: MyDatastore The diskformat can be thin,zeroedthick, or eagerzeroedthick. The datastore should be changed to specify the name of the datastore where you want your volumes created. Configure kubectl to point to your ICP instance. Deploy your new storage class kubectl create -f vsphere.yaml Now, when installing helm charts you can specify dynamic storage using the 'vsphere' storage class and it will dynamically provision your PV to the specified datastore.","title":"Configure vSphere for use by ICP"},{"location":"recipes/#recipe-7-integrating-with-an-existing-s3-storage-provider","text":"","title":"Recipe 7: Integrating with an existing S3 Storage Provider"}]}